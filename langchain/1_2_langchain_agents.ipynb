{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "5fd2253e",
   "metadata": {},
   "source": [
    "# Agents\n",
    "- 에이전트는 언어 모델과 도구를 결합하여 작업에 대해 추론하고, 사용할 도구를 결정하고, 반복적으로 솔루션을 향해 노력하는 시스템을 만듭니다.\n",
    "- create_agent생산에 바로 사용할 수 있는 에이전트 구현을 제공합니다.\n",
    "- LLM 에이전트는 목표를 달성하기 위해 루프 내에서 도구를 실행합니다 . \n",
    "- 에이전트는 중지 조건이 충족될 때까지, 즉 모델이 최종 출력을 내보내거나 반복 횟수 제한에 도달할 때까지 실행됩니다.\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a4f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model ¶\t에이전트의 언어 모델입니다. \n",
    "문자열 식별자(예: \"openai:gpt-4\") 또는 직접 채팅 모델 인스턴스(예: ChatOpenAI또는 다른 채팅 모델 )일 수 있습니다.\n",
    "지원되는 모델 문자열의 전체 목록은 을 참조하세요 init_chat_model.\n",
    "\n",
    "유형: str | BaseChatModel\n",
    "\n",
    "tools ¶\t도구 목록, dicts또는 Callable.\n",
    "None또는 빈 목록 인 경우 , 에이전트는 도구 호출 루프가 없는 모델 노드로 구성됩니다.\n",
    "\n",
    "유형: Sequence[BaseTool | Callable | dict[str, Any]] | None기본: None\n",
    "\n",
    "system_prompt ¶\tLLM에 대한 선택적 시스템 프롬프트입니다.\n",
    "프롬프트는 a로 변환되어 SystemMessage메시지 목록의 시작 부분에 추가됩니다.\n",
    "\n",
    "유형: str | None기본: None\n",
    "\n",
    "middleware ¶\t에이전트에 적용할 미들웨어 인스턴스의 시퀀스입니다.\n",
    "미들웨어는 다양한 단계에서 에이전트의 동작을 가로채고 수정할 수 있습니다. 전체 가이드를 참조하세요 .\n",
    "\n",
    "유형: Sequence[AgentMiddleware[StateT_co, ContextT]]기본: ()\n",
    "\n",
    "response_format ¶\t구조화된 응답을 위한 선택적 구성입니다.\n",
    "ToolStrategy, , 또는 Pydantic 모델 클래스가 될 수 있습니다 ProviderStrategy.\n",
    "\n",
    "제공된 경우, 에이전트는 대화 흐름 중에 구조화된 출력을 처리합니다. \n",
    "원시 스키마는 모델 기능에 따라 적절한 전략으로 래핑됩니다.\n",
    "\n",
    "유형: ResponseFormat[ResponseT] | type[ResponseT] | None기본: None\n",
    "\n",
    "state_schema ¶\tTypedDict.을 확장하는 선택적 스키마입니다 AgentState.\n",
    "이 스키마가 제공되면 AgentState미들웨어 상태 스키마와 병합할 때 기본 스키마 대신 사용됩니다. \n",
    "이를 통해 사용자는 사용자 지정 미들웨어를 생성하지 않고도 사용자 지정 상태 필드를 추가할 수 있습니다. \n",
    "일반적으로 state_schema관련 확장의 범위를 해당 후크/도구로 제한하기 위해 미들웨어를 통한 확장을 사용하는 것이 좋습니다.\n",
    "\n",
    "스키마는 .의 하위 클래스여야 합니다 AgentState[ResponseT].\n",
    "\n",
    "유형: type[AgentState[ResponseT]] | None기본: None\n",
    "\n",
    "context_schema ¶\t런타임 컨텍스트에 대한 선택적 스키마입니다.\n",
    "유형: type[ContextT] | None기본: None\n",
    "\n",
    "checkpointer ¶\t선택적 체크포인트 세이버 객체입니다.\n",
    "단일 스레드(예: 단일 대화)에 대한 그래프 상태(예: 채팅 메모리)를 유지하는 데 사용됩니다.\n",
    "\n",
    "유형: Checkpointer | None기본: None\n",
    "\n",
    "store ¶\t선택적 저장 객체입니다.\n",
    "여러 스레드(예: 여러 대화/사용자)에 걸쳐 데이터를 유지하는 데 사용됩니다.\n",
    "\n",
    "유형: BaseStore | None기본: None\n",
    "\n",
    "interrupt_before ¶\t중단하기 전에 노드 이름을 나열한 선택적 목록입니다.\n",
    "작업을 수행하기 전에 사용자 확인이나 기타 인터럽트를 추가하려는 경우에 유용합니다.\n",
    "\n",
    "유형: list[str] | None기본: None\n",
    "\n",
    "interrupt_after ¶\t인터럽트할 노드 이름의 선택적 목록입니다.\n",
    "출력에 직접 반환하거나 추가 처리를 실행하려는 경우에 유용합니다.\n",
    "\n",
    "유형: list[str] | None기본: None\n",
    "\n",
    "debug ¶\t그래프 실행에 대한 자세한 로깅을 활성화할지 여부입니다.\n",
    "활성화하면 에이전트 런타임 중 각 노드 실행, 상태 업데이트 및 전환에 대한 자세한 정보가 출력됩니다. \n",
    "미들웨어 동작 디버깅 및 에이전트 실행 흐름 이해에 유용합니다.\n",
    "\n",
    "유형: bool기본: False\n",
    "\n",
    "name ¶\t. 에 대한 선택적 이름입니다 CompiledStateGraph.\n",
    "이 이름은 에이전트 그래프를 다른 그래프에 하위 그래프 노드로 추가할 때 자동으로 사용됩니다. \n",
    "특히 다중 에이전트 시스템을 구축하는 데 유용합니다.\n",
    "\n",
    "유형: str | None기본: None\n",
    "\n",
    "cache ¶\tBaseCache그래프 실행 캐싱을 활성화하는 선택적 인스턴스입니다.\n",
    "유형: BaseCache | None기본: None\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d58d180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': {'messages': [AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 58, 'total_tokens': 73, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CbMHF7U5uHCjCVCaeDW9Cg1Thutjy', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--76173ff8-1487-4a89-b751-936f37e56187-0', tool_calls=[{'name': 'check_weather', 'args': {'location': 'San Francisco'}, 'id': 'call_mncn4MzXYLxqT8XHwerGvgvk', 'type': 'tool_call'}], usage_metadata={'input_tokens': 58, 'output_tokens': 15, 'total_tokens': 73, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "{'tools': {'messages': [ToolMessage(content=\"It's always sunny in San Francisco\", name='check_weather', id='453300fa-9736-40ec-b521-55941313eedc', tool_call_id='call_mncn4MzXYLxqT8XHwerGvgvk')]}}\n",
      "{'model': {'messages': [AIMessage(content='The weather in San Francisco is currently sunny.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 87, 'total_tokens': 97, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CbMHGEYH47nJ2VdliVdzKSxPAFHBT', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--ec4b05ae-6f2a-4f76-b1dc-bcb6bc727a9f-0', usage_metadata={'input_tokens': 87, 'output_tokens': 10, 'total_tokens': 97, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "에이전트 노드는 시스템 프롬프트를 적용한 후 메시지 목록을 사용하여 언어 모델을 호출합니다. \n",
    "결과에 AIMessage가 포함 되면 tool_calls그래프는 도구를 호출합니다. \n",
    "도구 노드는 도구를 실행하고 응답을 메시지 목록에 ToolMessage객체로 추가합니다. \n",
    "그런 다음 에이전트 노드는 언어 모델을 다시 호출합니다. \n",
    "tool_calls응답에 더 이상 존재하지 않을 때까지 이 프로세스가 반복됩니다. \n",
    "그런 다음 에이전트는 전체 메시지 목록을 반환합니다.\n",
    "\"\"\"\n",
    "from langchain.agents import create_agent\n",
    "def check_weather(location: str) -> str:\n",
    "    '''Return the weather forecast for the specified location.'''\n",
    "    return f\"It's always sunny in {location}\"\n",
    "\n",
    "graph = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[check_weather],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "inputs = {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    "for chunk in graph.stream(inputs, stream_mode=\"updates\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89f0989",
   "metadata": {},
   "source": [
    "# 동적 모델\n",
    "- 동적 모델은 다음에서 선택됩니다.\n",
    "- 현재 상태와 컨텍스트(맥락)에 따라 실행 시점(runtime)에 선택되는 모델을 말합니다.\n",
    "- 이렇게 하면 **복잡한 라우팅 로직(어떤 모델을 쓸지 자동으로 고르는 로직)**과 비용 최적화를 할 수 있습니다.\n",
    "- 다이내믹 모델을 사용하려면, @wrap_model_call 데코레이터를 사용해 미들웨어를 만들고,\n",
    "- 이 미들웨어 안에서 요청(request)에 사용되는 모델을 수정해 주면 됩니다.\n",
    "\n",
    "- 용어 정리\n",
    "    - runtime(실행 시점)\n",
    "        - 코드가 “돌아가고 있는 순간”에 그때그때 어떤 모델을 쓸지 결정한다는 의미.\n",
    "    - routing logic(라우팅 로직)\n",
    "        - “이 요청은 작은 모델로 처리해도 돼”\n",
    "        - “이건 중요한 요청이니까 큰 모델로 보내자”\n",
    "        - 이렇게 요청 → 적절한 모델로 보내는 규칙/로직.\n",
    "    - middleware(미들웨어)\n",
    "        - 요청이 실제 모델에 도달하기 전/후에 중간에서 요청·응답을 가공하는 층\n",
    "        - 여기서 “어떤 모델을 쓸지 바꿔 끼우는 역할”을 담당.\n",
    "    - @wrap_model_call 데코레이터\n",
    "        - 모델 호출 함수를 감싸서,\n",
    "        - 호출 전에 모델 선택/변경 로직을 넣을 수 있게 해주는 장치라고 보면 됨.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1e774a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handler(request)\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# 무엇: 체인을 이어서 실제 모델 호출을 수행.\u001b[39;00m\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# handler는 '다음 미들웨어 → 모델 호출' 파이프라인을 의미.\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# 반환값은 ModelResponse로, 이후 에이전트 루프가 계속 진행.\u001b[39;00m\n\u001b[32m     52\u001b[39m agent = create_agent(\n\u001b[32m     53\u001b[39m     model=basic_model,              \u001b[38;5;66;03m# 기본값(미들웨어가 없을 때 사용될 모델)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     tools=\u001b[43mtools\u001b[49m,                    \u001b[38;5;66;03m# 에이전트가 호출할 수 있는 툴 목록\u001b[39;00m\n\u001b[32m     55\u001b[39m     middleware=[dynamic_model_selection]  \u001b[38;5;66;03m# 위에서 정의한 미들웨어 적용(순서대로 실행)\u001b[39;00m\n\u001b[32m     56\u001b[39m )\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# 흐름 정리:\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m#  - 에이전트가 한 턴을 처리할 때마다 LLM 호출 직전에 dynamic_model_selection이 실행\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m#  - request.state[\"messages\"] 길이를 보고 현재 턴의 model을 결정\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m#  - 이후 툴콜/재호출 반복 중 매번 조건이 재평가되어 '턴별로' 모델이 달라질 수 있음\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'tools' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# 왜: OpenAI 채팅 모델을 LangChain 인터페이스로 사용하기 위해.\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "# 왜: 에이전트(툴콜 루프 포함) 그래프를 손쉽게 생성.\n",
    "\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "# 왜: LLM 호출 직전에 개입할 수 있는 미들웨어 유틸리티와 요청/응답 타입.\n",
    "#    - wrap_model_call: 미들웨어 함수 데코레이터\n",
    "#    - ModelRequest: 1회 LLM 호출의 요청 컨테이너(모델/메시지/상태 등)\n",
    "#    - ModelResponse: 1회 LLM 호출의 응답 컨테이너\n",
    "\n",
    "\n",
    "basic_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# 무엇: 기본(저비용/경량) 모델 인스턴스. 짧은 대화/간단 작업에 사용.\n",
    "\n",
    "advanced_model = ChatOpenAI(model=\"gpt-4o\")\n",
    "# 무엇: 고성능(고비용) 모델 인스턴스. 긴 대화/난이도 높은 추론에 사용.\n",
    "\n",
    "\n",
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"\n",
    "    왜: '모델 호출 직전'에 요청 내용을 보고 어떤 모델을 쓸지 동적으로 결정.\n",
    "    흐름:\n",
    "      (1) create_agent의 실행 중 LLM을 호출하려 할 때,\n",
    "      (2) 이 미들웨어가 request를 받음 → 조건 판단 후 request.model 교체,\n",
    "      (3) handler(request)로 다음 단계(실제 모델 호출)를 진행,\n",
    "      (4) 결과(ModelResponse)를 그대로 반환.\n",
    "    \"\"\"\n",
    "    message_count = len(request.state[\"messages\"])\n",
    "    # 무엇: 현재까지의 대화 길이를 기준으로 복잡도를 간단히 추정.\n",
    "    # 주의: '길이'는 토큰 수와 다름. 더 정확히 하려면 토큰 카운팅을 권장.\n",
    "\n",
    "    if message_count > 10:\n",
    "        # 조건: 대화가 길면(>10 메시지) 더 똑똑한 모델로 승급\n",
    "        model = advanced_model\n",
    "    else:\n",
    "        # 그 외에는 기본 모델 사용\n",
    "        model = basic_model\n",
    "\n",
    "    request.model = model\n",
    "    # 핵심: 이번 호출에서 사용할 모델을 '교체'한다.\n",
    "    # create_agent에 넘긴 기본 model보다 '이 값'이 우선 적용됨.\n",
    "\n",
    "    return handler(request)\n",
    "    # 무엇: 체인을 이어서 실제 모델 호출을 수행.\n",
    "    # handler는 '다음 미들웨어 → 모델 호출' 파이프라인을 의미.\n",
    "    # 반환값은 ModelResponse로, 이후 에이전트 루프가 계속 진행.\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=basic_model,              # 기본값(미들웨어가 없을 때 사용될 모델)\n",
    "    tools=tools,                    # 에이전트가 호출할 수 있는 툴 목록\n",
    "    middleware=[dynamic_model_selection]  # 위에서 정의한 미들웨어 적용(순서대로 실행)\n",
    ")\n",
    "# 흐름 정리:\n",
    "#  - 에이전트가 한 턴을 처리할 때마다 LLM 호출 직전에 dynamic_model_selection이 실행\n",
    "#  - request.state[\"messages\"] 길이를 보고 현재 턴의 model을 결정\n",
    "#  - 이후 툴콜/재호출 반복 중 매번 조건이 재평가되어 '턴별로' 모델이 달라질 수 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d39a9a",
   "metadata": {},
   "source": [
    "## 도구 (Tools)\n",
    "- 도구는 에이전트가 행동을 수행할 수 있게 해줍니다. 에이전트는 단순히 “모델에 도구를 한 번 바인딩”하는 수준을 넘어, 다음을 지원합니다\n",
    "    - 여러 개의 도구 호출을 순차적으로 실행(하나의 프롬프트로 트리거)\n",
    "    - 필요 시 병렬 도구 호출\n",
    "    - 이전 결과에 기반한 동적 도구 선택\n",
    "    - 도구 재시도 로직 및 오류 처리\n",
    "    - 도구 호출 사이의 상태 지속성(persistence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f75e09",
   "metadata": {},
   "source": [
    "## 도구 정의 \n",
    "- 에이전트에게 도구 목록을 전달합니다.\n",
    "- 빈 도구 목록이 제공되면 에이전트는 도구 호출 기능이 없는 단일 LLM 노드로 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4684f086",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWeather in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Sunny, 72°F\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# 실제 구현에선 외부 API 호출/에러 처리/단위 변환 등을 수행할 수 있음\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m agent = create_agent(\u001b[43mmodel\u001b[49m, tools=[search, get_weather])\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 무엇: 에이전트 그래프를 생성. `model`은 사용할 LLM(예: ChatOpenAI/Anthropic 등)\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 왜: tools 목록으로 @tool 데코레이터가 감싼 두 함수를 전달 → 에이전트가 필요 시 선택적으로 호출\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# 흐름(전체):\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m \u001b[38;5;66;03m#   4) LLM을 다시 호출해 툴 결과를 반영한 최종 답변 생성\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m#   5) 같은 방식으로 `get_weather`도 호출 가능. 더 이상 tool_calls 없으면 종료\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "# 무엇: 일반 파이썬 함수를 LangChain \"Tool\" 프로토콜로 래핑하는 데코레이터를 가져옴\n",
    "# 왜: 에이전트가 LLM 응답 중 tool_calls를 내보냈을 때, 이 함수들을 실제로 실행 가능하게 하려는 것\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "# 무엇: LLM + (필요 시) 도구 호출 루프를 자동으로 처리하는 에이전트를 생성하는 팩토리\n",
    "# 흐름: 모델 응답 → tool_calls 있으면 해당 툴 실행 → 결과를 메시지로 합치고 재호출… 반복\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for information.\"\"\"\n",
    "    # 왜: 이 함수의 docstring은 툴 설명(description)으로 사용됨 → LLM이 언제/어떻게 써야 할지 힌트 제공\n",
    "    # 무엇: 문자열 인자 `query`를 받아 \"검색 결과\"처럼 보이는 응답을 돌려주는 데모용 툴\n",
    "    return f\"Results for: {query}\"\n",
    "    # 흐름: 에이전트가 tool_calls로 {\"tool_name\":\"search\",\"arguments\":{\"query\":\"...\"}}\n",
    "    #       를 요청하면 이 함수가 실행되고 문자열 결과가 ToolMessage로 대화 히스토리에 추가됨\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get weather information for a location.\"\"\"\n",
    "    # 무엇: 특정 위치의 날씨 정보를 반환하는 데모용 툴\n",
    "    # 왜: 타입힌트(location: str) 덕분에 LLM이 인자 형태를 더 정확하게 추론/직렬화할 수 있음\n",
    "    return f\"Weather in {location}: Sunny, 72°F\"\n",
    "    # 실제 구현에선 외부 API 호출/에러 처리/단위 변환 등을 수행할 수 있음\n",
    "\n",
    "\n",
    "agent = create_agent(model, tools=[search, get_weather])\n",
    "# 무엇: 에이전트 그래프를 생성. `model`은 사용할 LLM(예: ChatOpenAI/Anthropic 등)\n",
    "# 왜: tools 목록으로 @tool 데코레이터가 감싼 두 함수를 전달 → 에이전트가 필요 시 선택적으로 호출\n",
    "# 흐름(전체):\n",
    "#   1) 사용자 프롬프트 입력 → LLM 호출\n",
    "#   2) LLM이 판단: \"검색\" 필요? → tool_calls에 'search' 호출을 기재\n",
    "#   3) 프레임워크가 실제 `search()` 실행 → 결과를 ToolMessage로 messages에 합침\n",
    "#   4) LLM을 다시 호출해 툴 결과를 반영한 최종 답변 생성\n",
    "#   5) 같은 방식으로 `get_weather`도 호출 가능. 더 이상 tool_calls 없으면 종료\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5982bf23",
   "metadata": {},
   "source": [
    "## 도구 오류 처리(Tool error handling)\n",
    "- 도구 오류가 처리되는 방식을 사용자 지정하려면 \n",
    "- @wrap_tool_call데코레이터를 사용하여 미들웨어를 만듭니다.\n",
    "- ToolMessage도구가 실패하면 에이전트는 사용자 지정 오류 메시지와 함께 다음을 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd2c1d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_tool_call\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "@wrap_tool_call\n",
    "def handle_tool_errors(request, handler):\n",
    "    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n",
    "    try:\n",
    "        return handler(request)\n",
    "    except Exception as e:\n",
    "        # Return a custom error message to the model\n",
    "        return ToolMessage(\n",
    "            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n",
    "            tool_call_id=request.tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[search, get_weather],\n",
    "    middleware=[handle_tool_errors]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19ed5021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [\n",
    "#     ...\n",
    "#     ToolMessage(\n",
    "#         content=\"Tool error: Please check your input and try again. (division by zero)\",\n",
    "#         tool_call_id=\"...\"\n",
    "#     ),\n",
    "#     ...\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bdc1095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "# 무엇: LLM + (툴 호출 루프) 에이전트를 만들어주는 팩토리.\n",
    "# 흐름: 모델 응답 → tool_calls 있으면 툴 실행 → 결과(또는 에러)를 메시지에 합쳐 다시 모델 호출… 반복.\n",
    "\n",
    "from langchain.agents.middleware import wrap_tool_call\n",
    "# 무엇: \"툴을 실제 실행하는 순간\"을 가로채는 미들웨어 데코레이터.\n",
    "# 왜: 툴 에러 공통 처리(재시도/로그/마스킹)를 중앙집중식으로 넣기 위함.\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "# 무엇: \"툴 호출의 결과\"를 모델에게 전달하는 메시지 타입.\n",
    "# 왜: LLM은 tool_calls로 툴을 요청하고, 에이전트는 ToolMessage로 해당 호출의 결과(성공/실패)를 연결해 돌려줌.\n",
    "\n",
    "\n",
    "@wrap_tool_call\n",
    "def handle_tool_errors(request, handler):\n",
    "    \"\"\"\n",
    "    왜: 툴 실행 시 발생하는 모든 예외를 여기서 잡아\n",
    "        모델이 이해할 수 있는 '안전한 에러 메시지'로 반환(누설 방지)하려고.\n",
    "    무엇: wrap_tool_call로 감싼 함수는 각 툴 실행마다 호출되는 '미들웨어 훅'.\n",
    "    인자:\n",
    "      - request: 툴 실행 컨텍스트(어떤 툴, 인자, tool_call 메타 등)\n",
    "      - handler: 실제 툴을 실행하는 콜백. (request -> ToolMessage 반환)\n",
    "    반환:\n",
    "      - 일반적으로 handler(request)의 결과(ToolMessage)\n",
    "      - 에러 시: 커스텀 ToolMessage(에러 설명)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return handler(request)  # 흐름: 정상 케이스 → 실제 툴을 실행하고 결과를 그대로 반환\n",
    "    except Exception as e:\n",
    "        # 무엇: 모든 예외를 잡아 모델 친화적인 에러 메시지로 변환\n",
    "        # 왜: 스택트레이스/민감정보를 그대로 노출하지 않기 위해 메시지를 '마스킹'해서 전달\n",
    "        return ToolMessage(\n",
    "            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n",
    "            # 무엇: tool_call_id는 \"어떤 툴 호출에 대한 응답인지\"를 모델이 매칭할 수 있게 하는 식별자\n",
    "            # 왜: 모델은 여러 병렬/연속 툴 호출을 할 수 있으므로, 응답을 정확히 연결해야 함\n",
    "            tool_call_id=request.tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",                 # 무엇: 에이전트가 기본으로 사용할 LLM\n",
    "    tools=[search, get_weather],    # 무엇: LLM이 호출할 수 있는 툴 목록(@tool로 래핑된 함수들)\n",
    "    middleware=[handle_tool_errors] # 무엇: 위에서 만든 툴-에러 미들웨어를 적용\n",
    ")\n",
    "# 전체 흐름:\n",
    "# 1) 모델이 tool_calls로 예: {\"id\":\"call-1\",\"name\":\"search\",\"args\":{\"query\":\"...\"}}\n",
    "# 2) 에이전트가 search 툴 실행 직전에 handle_tool_errors 미들웨어 호출\n",
    "# 3) 정상: handler(request) → ToolMessage(content=\"Results...\", tool_call_id=\"call-1\")\n",
    "#    에러: except → ToolMessage(content=\"Tool error: ...\", tool_call_id=\"call-1\")\n",
    "# 4) 이 ToolMessage가 messages에 추가되고, 모델은 그 결과(또는 에러)를 읽고 다음 행동(재시도/다른 툴/최종답변)을 결정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4709c41",
   "metadata": {},
   "source": [
    "## ReAct루프에서의 도구 사용(Tool use in the ReAct loop)\n",
    "- 에이전트는 ReAct(\"추론 + 행동\") 패턴을 따르며, \n",
    "- 간단한 추론 단계와 타겟 도구 호출을 번갈아가며 수행하고, \n",
    "- 최종 답변을 제공할 때까지 결과 관찰 결과를 후속 결정에 반영합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac00e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================== 사용자 메시지 ==============================\n",
    "# 지금 가장 인기 있는 무선 헤드폰을 찾아보고, 재고가 있는지 확인해줘.\n",
    "\n",
    "# 추론: 인기는 시간에 따라 변하므로 제공된 검색 도구를 사용해야 한다.\n",
    "# 행동: call search_products({\"query\": \"wireless headphones\"})\n",
    "\n",
    "# =============================== AI 메시지 ================================\n",
    "# 도구 호출:\n",
    "#   search_products (call_abc123)\n",
    "#   인자:\n",
    "#     query: \"wireless headphones\"\n",
    "\n",
    "# =============================== 도구 메시지 ===============================\n",
    "# \"wireless headphones\"와 일치하는 제품 5개를 찾았습니다.\n",
    "# 상위 5개: WH-1000XM5, ...\n",
    "\n",
    "# 추론: 답변하기 전에 상위 항목의 재고를 확인해야 한다.\n",
    "# 행동: call check_inventory({\"product_id\": \"WH-1000XM5\"})\n",
    "\n",
    "# =============================== AI 메시지 ================================\n",
    "# 도구 호출:\n",
    "#   check_inventory (call_def456)\n",
    "#   인자:\n",
    "#     product_id: \"WH-1000XM5\"\n",
    "\n",
    "# =============================== 도구 메시지 ===============================\n",
    "# 제품 WH-1000XM5: 재고 10개\n",
    "\n",
    "# 추론: 가장 인기 있는 모델과 재고 현황을 파악했다. 이제 최종 답변을 작성한다.\n",
    "# 행동: (최종)\n",
    "\n",
    "# =============================== AI 메시지 ================================\n",
    "# 인기 무선 헤드폰 WH-1000XM5를 찾았고, 현재 재고는 10개입니다. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154bb2b1",
   "metadata": {},
   "source": [
    "## 시스템 프롬프트\n",
    "- 프롬프트를 제공하여 에이전트가 작업에 접근하는 방식을 구체화할 수 있습니다. \n",
    "- system_prompt 매개변수는 문자열로 제공될 수 있습니다.\n",
    "- system_prompt가 제공안되면 에이전트는 메시지에서 직접 작업을 추론합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc986652",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model,\n",
    "    tools,\n",
    "    system_prompt=\"You are a helpful assistant. Be concise and accurate.\"\n",
    "# 무엇: 시스템 메시지(역할/톤/정책). create_agent가 SystemMessage로 변환해 대화 맨 앞에 고정 삽입.\n",
    "# 왜: “간결하고 정확하게” 답하라는 전역 규칙을 모델에 강하게 주입.\n",
    "# 흐름: [SystemMessage] + [대화 히스토리] → LLM 호출 → (필요 시) 도구 호출 → 최종 답변.\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "모델 파라미터 명시: 재현성을 위해 temperature=0.2 같은 하이퍼파라미터를 model 생성 시 지정하세요.\n",
    "툴 스키마: 툴의 인자 타입힌트 + docstring을 정확히 써야 모델이 인자를 제대로 직렬화합니다.\n",
    "메모리/중단(HITL): 장기 대화나 승인 게이트가 필요하면 \n",
    "checkpointer=(예: InMemorySaver/DB Saver)와 HumanInTheLoop 미들웨어를 함께 고려하세요.\n",
    "요약(Summary): 대화가 길어지면 SummarizationMiddleware(...)로 토큰/비용을 관리하세요.\n",
    "검증(Structured Output): 스키마가 필요한 작업이면 \n",
    "response_format=YourSchema로 구조화 출력을 강제하면 다운스트림 로직이 안정적입니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6bdce1",
   "metadata": {},
   "source": [
    "## 동적 시스템 프롬프트\n",
    "- 런타임 컨텍스트나 에이전트 상태에 따라 시스템 프롬프트를 수정해야 하는 고급 사용 사례의 경우 미들웨어를 사용할 수 있습니다 .\n",
    "- 데코레이터 @dynamic_prompt는 모델 요청에 따라 동적으로 시스템 프롬프트를 생성하는 미들웨어를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "192a7c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "머신러닝은 컴퓨터에게 데이터를 통해 특정 작업을 수행하는 법을 배우게 하는 기술입니다. 간단히 말하자면, 컴퓨터가 사람처럼 스스로 경험을 통해 학습할 수 있도록 돕는 것입니다.\n",
      "\n",
      "예를 들어, 이메일 서비스를 사용하는데 스팸 메일을 자동으로 분류해주는 기능이 있다고 해봅시다. 머신러닝 기법을 사용하면, 컴퓨터는 많은 이메일 데이터를 보고 어떤 것이 스팸이고 어떤 것이 정당한 메일인지 스스로 배우게 됩니다. 그렇게 되면 컴퓨터가 새로 들어오는 이메일이 스팸인지 아닌지를 자동으로 구분할 수 있게 됩니다.\n",
      "\n",
      "머신러닝은 사람의 개입 없이 스스로 데이터를 분석해서 패턴을 찾아내고, 그 패턴을 바탕으로 예측이나 결정을 내리는 데에 사용됩니다. 이는 다양한 분야에서 활용되고 있으며, 이미지 인식, 음성 인식, 추천 시스템 등 다양한 곳에서 머신러닝 기술이 쓰이고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "# 무엇: 딕셔너리 형태의 스키마(키-값 타입 명세)를 만들기 위한 표준 타입 도구.\n",
    "# 왜: 런타임 컨텍스트의 \"모양\"을 코드 레벨에서 명확히 선언해 타입 안정성을 높임.\n",
    "# 흐름: 아래 Context 클래스 정의에 사용 → create_agent(context_schema=Context)로 이어짐.\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "# 무엇: LLM + (필요 시) 도구 호출 루프를 손쉽게 구성하는 팩토리 함수.\n",
    "# 흐름: model/system_prompt/tools/middleware/context_schema 등을 받아 '에이전트 그래프'를 생성.\n",
    "\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "# 무엇:\n",
    "#  - dynamic_prompt: \"시스템 프롬프트를 동적으로 생성/치환\"하는 미들웨어 데코레이터.\n",
    "#  - ModelRequest: 미들웨어 훅이 받는 요청 컨테이너 타입 (모델, 메시지, 상태, runtime 등 포함).\n",
    "# 왜: 모델 호출 직전에 프롬프트를 가로채 가공할 수 있게 해줌.\n",
    "# 흐름: @dynamic_prompt로 감싼 함수가 create_agent의 middleware로 들어가고,\n",
    "#       매 턴 LLM 호출 직전에 호출되어 system prompt를 결정/치환함.\n",
    "\n",
    "\n",
    "class Context(TypedDict):\n",
    "    user_role: str\n",
    "# 무엇: 런타임 컨텍스트의 스키마(형태)를 정의. 여기서는 user_role만 필요.\n",
    "# 왜: 에이전트 실행 시 \"context={\"user_role\":\"expert\"}\"와 같은 값을 안전하게 전달/검증.\n",
    "# 흐름: create_agent(context_schema=Context)에 전달 → agent.invoke(context=...)에서 타입 일치 보장.\n",
    "\n",
    "\n",
    "@dynamic_prompt\n",
    "def user_role_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"Generate system prompt based on user role.\"\"\"\n",
    "    # 무엇: dynamic_prompt 미들웨어 함수. LLM 호출 직전에 불려 '시스템 프롬프트' 문자열을 반환.\n",
    "    # 왜: 사용자 역할(expert/beginner/user)에 따라 말투/깊이/스타일을 달리 하기 위함.\n",
    "    # 흐름: create_agent(middleware=[user_role_prompt])로 등록 → 각 턴 시작 시 호출되어\n",
    "    #       반환한 문자열이 SystemMessage로 주입됨(기존 system_prompt를 대체하거나 결합, 구현에 따라 다름).\n",
    "\n",
    "    user_role = request.runtime.context.get(\"user_role\", \"user\")\n",
    "    # 무엇: 호출 시점의 런타임 컨텍스트에서 user_role을 읽음. 없으면 \"user\" 기본값.\n",
    "    # 왜: 미들웨어가 외부(호출자)로부터 상황 정보를 받아 프롬프트를 다이나믹하게 만들기 위함.\n",
    "    # 흐름: agent.invoke(..., context={\"user_role\":\"expert\"}) → 여기에서 사용.\n",
    "\n",
    "    base_prompt = \"You are a helpful assistant.\"\n",
    "    # 무엇: 공통 베이스 문구(역할/톤의 기본 규칙).\n",
    "    # 왜: 공통 정책은 유지하되, 아래에서 역할별 문구를 덧붙이는 패턴으로 재사용성↑.\n",
    "\n",
    "    if user_role == \"expert\":\n",
    "        return f\"{base_prompt} Provide detailed technical responses.\"\n",
    "        # 무엇: 전문가 모드 → 기술적/깊이 있는 설명 지향.\n",
    "        # 흐름: 반환 즉시 이 문자열이 이번 턴의 SystemMessage로 주입되어 모델 거동에 영향.\n",
    "\n",
    "    elif user_role == \"beginner\":\n",
    "        return f\"{base_prompt} Explain concepts simply and avoid jargon.\"\n",
    "        # 무엇: 초보자 모드 → 쉬운 설명, 전문용어 최소화.\n",
    "\n",
    "    return base_prompt\n",
    "    # 무엇: 그 외(기본) 모드 → 베이스 문구 그대로 사용.\n",
    "    # 팁: 역할이 더 늘면 elif 블록으로 확장(executive, manager, designer 등).\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    # 무엇: 기본으로 사용할 LLM(문자열 식별자 또는 ChatOpenAI 인스턴스).\n",
    "    # 왜: 각 턴의 응답 생성/툴콜 판단의 중심.\n",
    "\n",
    "    # tools=[web_search],\n",
    "    # 무엇: LLM이 필요 시 호출할 수 있는 도구 목록(예: 웹 검색).\n",
    "    # 흐름: 모델이 tool_calls를 제안 → 프레임워크가 실제 실행 → 결과를 ToolMessage로 합치고 재호출.\n",
    "\n",
    "    middleware=[user_role_prompt],\n",
    "    # 무엇: 위에서 정의한 dynamic_prompt 미들웨어를 장착.\n",
    "    # 왜: 매 턴 모델 호출 직전에 '역할 기반 시스템 프롬프트'로 치환/주입.\n",
    "    # 흐름: messages 빌드 → (미들웨어) system prompt 생성/주입 → LLM 호출 → (필요 시) 툴 호출.\n",
    "\n",
    "    context_schema=Context\n",
    "    # 무엇: 컨텍스트 스키마를 등록해, invoke(context=...)로 들어올 키/타입을 명시.\n",
    "    # 왜: 잘못된 컨텍스트(키 누락/타입 오류)를 조기에 방지.\n",
    "    # 흐름: 아래 agent.invoke에서 context={\"user_role\":\"expert\"}가 들어오면 이 스키마로 적합성 보장.\n",
    ")\n",
    "\n",
    "# The system prompt will be set dynamically based on context\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"머신러닝이란?\"}]},\n",
    "    # 무엇: 사용자 입력(질문). create_agent 규약상 messages 리스트로 전달.\n",
    "    # 흐름: 이 입력 + (미들웨어 생성한 SystemMessage) → LLM 호출 → (필요 시) web_search 도구 호출 → 최종 응답.\n",
    "\n",
    "    context={\"user_role\": \"beginner\"}\n",
    "    # 무엇: 이번 호출의 런타임 컨텍스트. 미들웨어에서 request.runtime.context로 접근 가능.\n",
    "    # 왜: 역할을 expert로 지정하면, 미들웨어가 \"기술적으로 자세히\" 설명하는 시스템 프롬프트를 구성.\n",
    "    # 흐름: user_role_prompt → \"Provide detailed technical responses.\"가 붙은 SystemMessage 생성.\n",
    ")\n",
    "# 결과: result에는 메시지 히스토리와 최종 AIMessage(전문가 톤의 ML 설명)가 포함.\n",
    "\n",
    "print(result['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f27c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bce158b5",
   "metadata": {},
   "source": [
    "## Invocation\n",
    "- 에이전트는 State(상태) 에 “업데이트”를 전달해서 호출할 수 있습니다.\n",
    "- 모든 에이전트의 상태에는 메시지들의 시퀀스가 포함되어 있으며,\n",
    "- 에이전트를 호출하려면 여기에 새 메시지를 추가해서 전달하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a868e86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}]}\n",
    ")\n",
    "\"\"\"\n",
    "에이전트에서 단계(step)별 실행 과정이나 토큰을 스트리밍 형태로 받고 싶다면,\n",
    "스트리밍 가이드를 참고하세요.\n",
    "\n",
    "그 외에는, 이 에이전트는 LangGraph의 Graph API를 따르며,\n",
    "stream 과 invoke 같은 관련 메서드들을 모두 지원합니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a373643b",
   "metadata": {},
   "source": [
    "## Advanced concepts (고급 개념)\n",
    "### Structured output (구조화된 출력)\n",
    "- 어떤 상황에서는, 에이전트가 특정 형식(schema)에 맞춰 결과를 반환하길 원할 수 있습니다.\n",
    "- LangChain은 response_format 파라미터를 통해 구조화된 출력(Structured output)을 제공하는 전략들을 지원합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3908f0c0",
   "metadata": {},
   "source": [
    "### ToolStrategy\n",
    "\n",
    "- ToolStrategy 는 인공적인(tool-based) 도구 호출을 활용하여 구조화된 출력을 생성합니다.\n",
    "- 이 방식은 도구 호출을 지원하는 어떤 모델이든 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "615fff40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    name: str\n",
    "    email: str\n",
    "    phone: str\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    # tools=[search_tool],\n",
    "    response_format=ToolStrategy(ContactInfo)\n",
    ")\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n",
    "})\n",
    "\n",
    "result[\"structured_response\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0cf8e0",
   "metadata": {},
   "source": [
    "### ProviderStrategy\n",
    "- ProviderStrategy는 모델 제공자(예:OpenAI)가 제공하는 네이티브 structured output 기능을 사용\n",
    "- 이 방식은 보다 신뢰도가 높지만, 해당 기능을 네이티브로 지원하는 제공자에서만 사용 할수 있습니다.(예:OpenAI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73b3b316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Extract contact info from: Jane Smith', additional_kwargs={}, response_metadata={}, id='575bb582-04f1-4c8a-a4dd-f82a58935049'),\n",
       "  AIMessage(content='{\"name\":\"Jane Smith\",\"email\":\"Not provided\",\"phone\":\"Not provided\"}', additional_kwargs={'parsed': None, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 84, 'total_tokens': 105, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_b1442291a8', 'id': 'chatcmpl-CbMsTmqqJRnQzPmV7LIdDCdOMlPks', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--dbd3c32d-d659-402b-bc9e-35ab1a0e558b-0', usage_metadata={'input_tokens': 84, 'output_tokens': 21, 'total_tokens': 105, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " 'structured_response': ContactInfo(name='Jane Smith', email='Not provided', phone='Not provided')}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents.structured_output import ProviderStrategy\n",
    "\n",
    "agent = create_agent(\n",
    "    model = \"gpt-4o\",\n",
    "    response_format=ProviderStrategy(ContactInfo)\n",
    ")\n",
    "\"\"\"\n",
    "langchain 1.0 기준으로,\n",
    "단순히 response_format=ContactInfo 처럼 schema만 바로 넘기는 방식은 더 이상 지원되지 않습니다.\n",
    "반드시 ToolStrategy 또는 ProviderStrategy 를 명시적으로 사용해야 합니다.\n",
    "\n",
    "구조화된 출력에 대한 더 자세한 내용은 Structured output 문서를 참고하세요.\n",
    "\"\"\"\n",
    "agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: Jane Smith\"}]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d5491a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 원본 텍스트 ===\n",
      "연락처: John Doe, john@example.com, (555) 123-4567\n",
      "\n",
      "=== 구조화된 출력 (ContactInfo) ===\n",
      "name='John Doe' email='john@example.com' phone='(555) 123-4567'\n",
      "<class '__main__.ContactInfo'>\n",
      "\n",
      "=== 필드별 접근 ===\n",
      "이름 : John Doe\n",
      "이메일 : john@example.com\n",
      "전화번호 : (555) 123-4567\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "from pydantic import BaseModel\n",
    "# 무엇: 구조화된 출력의 \"스키마\"를 정의하기 위해 사용하는 모델 베이스 클래스.\n",
    "# 왜: ContactInfo 같은 형태(필드/타입)를 명확히 선언 → LLM 응답을 여기에 맞춰 강제.\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ProviderStrategy\n",
    "# 무엇: \"모델 제공자의 네이티브 구조화 출력 기능\"을 사용하는 전략.\n",
    "# 왜: OpenAI(gpt-4o 등)의 JSON Schema 기반 structured output을 직접 활용하기 위해.\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "# 무엇: OpenAI 채팅 모델 래퍼. 문자열 \"gpt-4o\" 대신 인스턴스로도 넘길 수 있음.\n",
    "\n",
    "# 1) 구조화 출력 스키마 정의 -------------------------------------\n",
    "class ContactInfo(BaseModel):\n",
    "    \"\"\"연락처 정보 스키마\"\"\"\n",
    "    name: str\n",
    "    email: str\n",
    "    phone: str\n",
    "\n",
    "# 2) LLM 인스턴스 생성 -------------------------------------------\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    "    temperature=0,\n",
    ")\n",
    "# 왜 인스턴스를 쓰냐?\n",
    "# - 나중에 max_tokens, timeout, extra_headers 등 더 세밀하게 제어할 수 있음.\n",
    "# - 문자열로 model만 넘기는 것도 가능하지만, 인스턴스를 쓰면 확장성이 좋음.\n",
    "\n",
    "\n",
    "# 3) Agent 생성: ProviderStrategy로 구조화 출력 활성화 ----------\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    # 또는 model=\"gpt-4o\" 도 가능하지만, 일단 llm 인스턴스를 넘겼다고 가정.\n",
    "\n",
    "    # 여기 포인트: response_format=ProviderStrategy(ContactInfo)\n",
    "    response_format=ProviderStrategy(ContactInfo),\n",
    "    # 무엇:\n",
    "    #   - \"이번 에이전트는 응답을 ContactInfo 형태로 구조화해서 돌려줘\"\n",
    "    #   - ProviderStrategy는 OpenAI의 native structured output (JSON schema)을 사용.\n",
    "    # 흐름:\n",
    "    #   - agent.invoke(...) 호출 시,\n",
    "    #   - LangChain이 OpenAI API 요청에 JSON Schema를 같이 보내서\n",
    "    #   - 모델이 ContactInfo에 맞는 JSON을 생성하도록 유도.\n",
    ")\n",
    "\n",
    "# 4) 호출 예시 ---------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    user_text = \"연락처: John Doe, john@example.com, (555) 123-4567\"\n",
    "    result: dict[str, Any] = agent.invoke(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": (\n",
    "                        \"다음 문장에서 연락처 정보를 추출해서 \"\n",
    "                        \"ContactInfo 스키마에 맞춰줘.\\n\\n\"\n",
    "                        f\"{user_text}\"\n",
    "                    ),\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    # result 안에는 원래대로 messages도 있고,\n",
    "    # ProviderStrategy를 썼기 때문에 structured_response도 추가로 들어 있음.\n",
    "    structured: ContactInfo = result[\"structured_response\"]\n",
    "\n",
    "    print(\"=== 원본 텍스트 ===\")\n",
    "    print(user_text)\n",
    "\n",
    "    print(\"\\n=== 구조화된 출력 (ContactInfo) ===\")\n",
    "    print(structured)           # ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n",
    "    print(type(structured))     # <class '__main__.ContactInfo'>\n",
    "\n",
    "    print(\"\\n=== 필드별 접근 ===\")\n",
    "    print(\"이름 :\", structured.name)\n",
    "    print(\"이메일 :\", structured.email)\n",
    "    print(\"전화번호 :\", structured.phone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e80bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512743a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d1aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b67e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cdcded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9907430c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401f7c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d90e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a6e6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f210114a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00dd861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d73cfa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c461d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (phase1_venv)",
   "language": "python",
   "name": "phase1_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
