{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f0d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langgraph langchain-openai langchain-chroma langchain-community python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac7dae",
   "metadata": {},
   "source": [
    "# Build a basic agent\n",
    "- 질문에 답하고 도구를 호출할 수 있는 간단한 에이전트를 만드는 것부터 시작하세요. 에이전트는 언어 모델로 gpt-4o-mini를 사용하고, 도구로 기본적인 날씨 함수를 사용하며, 간단한 프롬프트를 통해 동작을 안내합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5dccfc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='what is the weather in sf', additional_kwargs={}, response_metadata={}, id='54e9cc19-511f-4d84-bd2e-6a69bbfd27ab'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 60, 'total_tokens': 75, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CYnVJmhnGQgcmaZntVb4lwN41fAGt', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--3f58bf45-543a-454e-b551-cca7975a8ced-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'San Francisco'}, 'id': 'call_gex7Iulo2fFPWoyHRJmCaqSY', 'type': 'tool_call'}], usage_metadata={'input_tokens': 60, 'output_tokens': 15, 'total_tokens': 75, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
      "              ToolMessage(content=\"It's always sunny in San Francisco!\", name='get_weather', id='f40d7cbc-8238-46a8-8f30-9d5ea7c3c4cb', tool_call_id='call_gex7Iulo2fFPWoyHRJmCaqSY'),\n",
      "              AIMessage(content='샌프란시스코의 날씨는 항상 맑습니다!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 90, 'total_tokens': 107, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CYnVKlvUdZ502dgXoLHlXMwDrsKft', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--0f08c984-238b-4599-8a3f-9c6a4fcd332b-0', usage_metadata={'input_tokens': 90, 'output_tokens': 17, 'total_tokens': 107, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "import pprint\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful assistant, please answer korean.\"\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "answer = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")\n",
    "pprint.pprint(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a5089",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_agent(\n",
    "    model: str | BaseChatModel,\n",
    "    tools: Sequence[BaseTool | Callable | dict[str, Any]] | None = None,\n",
    "    *,\n",
    "    system_prompt: str | None = None,\n",
    "    middleware: Sequence[AgentMiddleware[StateT_co, ContextT]] = (),\n",
    "    response_format: ResponseFormat[ResponseT] | type[ResponseT] | None = None,\n",
    "    state_schema: type[AgentState[ResponseT]] | None = None,\n",
    "    context_schema: type[ContextT] | None = None,\n",
    "    checkpointer: Checkpointer | None = None,\n",
    "    store: BaseStore | None = None,\n",
    "    interrupt_before: list[str] | None = None,\n",
    "    interrupt_after: list[str] | None = None,\n",
    "    debug: bool = False,\n",
    "    name: str | None = None,\n",
    "    cache: BaseCache | None = None,\n",
    ") -> CompiledStateGraph[\n",
    "    AgentState[ResponseT], ContextT, _InputAgentState, _OutputAgentState[ResponseT]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb932b3",
   "metadata": {},
   "source": [
    "## 시스템 프롬프트정의\n",
    "- 시스템 프롬프트는 상담원의 역할과 행동을 정의합니다. 구체적이고 실행 가능한 메세지를 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfc8c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns.\n",
    "                   you always have to speak in korean.\n",
    "You have access to two tools:\n",
    "\n",
    "- get_weather_for_location: use this to get the weather for a specific location\n",
    "- get_user_location: use this to get the user's location\n",
    "\n",
    "If a user asks you for the weather, make sure you know the location. \n",
    "If you can tell from the question that they mean wherever they are, \n",
    "use the get_user_location tool to find their location.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280f1e75",
   "metadata": {},
   "source": [
    "## 도구 만들기\n",
    "- 도구는 사용자가 정의한 함수를 호출하여 모델이 외부 시스템과 상호 작용할 수 있도록 합니다. \n",
    "- 도구는 런타임 컨텍스트 에 따라 달라질 수 있으며, 에이전트 메모리 와도 상호 작용할 수 있습니다 .\n",
    "- 아래에서 get_user_location도구가 런타임 컨텍스트를 사용하는 방법을 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a62f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n도구는 잘 문서화되어야 합니다. \\n도구의 이름, 설명, 인수 이름은 모델 프롬프트의 일부가 됩니다. \\nLangChain의 @tool데코레이터는 메타데이터를 추가하고 \\nToolRuntime매개변수를 통한 런타임 주입을 활성화합니다.\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 표준 라이브러리 dataclasses에서 dataclass 데코레이터를 가져옵니다.\n",
    "# 왜? - 간단한 데이터 보관용 클래스(런타임 컨텍스트 등)를 보일러플레이트 없이 정의하기 위해.\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# LangChain의 Tool 관련 유틸을 불러옵니다.\n",
    "# @tool: 일반 파이썬 함수를 에이전트가 호출할 수 있는 \"도구\"로 래핑해주는 데코레이터\n",
    "# ToolRuntime: 툴 실행 시 주입되는 실행 컨텍스트(사용자 정의 타입으로 제네릭 지정 가능)\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "\n",
    "# === Tool 1: 날씨 조회 ===\n",
    "@tool\n",
    "def get_weather_for_location(city: str) -> str:\n",
    "    \"\"\"\n",
    "    이 툴의 문서 문자열(docstring).\n",
    "    왜? - LangChain이 툴의 설명(description)으로 활용하여 LLM이 '언제/어떻게' 써야 할지 힌트를 줌.\n",
    "    내용: '특정 도시의 날씨를 알려준다'는 목적을 명시.\n",
    "    \"\"\"\n",
    "    # 함수 인자: city (str)\n",
    "    # 왜? - LLM이 툴을 호출할 때 JSON 인자를 생성하므로, 명확한 타입 힌팅/이름이 매우 중요.\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "    # 반환: 간단한 문자열 응답 (데모용)\n",
    "    # 실제 구현에선 외부 API 호출/캐시/에러처리 등을 수행할 수 있음.\n",
    "    # 흐름: 에이전트 -> (툴콜 결심) -> get_weather_for_location(city=...) 실행 -> 문자열 결과를 ToolMessage로 메시지 스택에 추가.\n",
    "\n",
    "\n",
    "# === 런타임 컨텍스트 정의 ===\n",
    "@dataclass\n",
    "class Context:\n",
    "    \"\"\"\n",
    "    사용자 정의 '런타임 컨텍스트' 스키마.\n",
    "    왜 dataclass? - 가볍고 불변성/타입힌트가 명시적인 컨테이너를 쉽게 정의.\n",
    "    여기서는 user_id 하나만 담지만, 실전에서는 권한정보, 세션, 테넌트, 지역(locale) 등 확장 가능.\n",
    "    \"\"\"\n",
    "    user_id: str  # 이 컨텍스트가 가지고 다닐 정보(예: 인증된 사용자 ID)\n",
    "\n",
    "\n",
    "# === Tool 2: 컨텍스트를 읽는 툴 ===\n",
    "@tool\n",
    "def get_user_location(runtime: ToolRuntime[Context]) -> str:\n",
    "    \"\"\"\n",
    "    이 툴은 '런타임 컨텍스트'를 입력으로 받습니다.\n",
    "    왜? - 툴 실행 시점에 시스템이 주입한 Context(예: user_id)를 참조하여\n",
    "          사용자 맞춤 동작을 수행하려고.\n",
    "    문서 문자열은 LLM에게 '이 툴은 user_id 기반 사용자 정보를 조회'한다는 사용 의도를 제공합니다.\n",
    "    \"\"\"\n",
    "    # ToolRuntime[Context]: 툴 호출 당시의 실행 환경 + context를 접근하게 해줌.\n",
    "    # 제네릭에 우리가 정의한 Context 타입을 넣어, runtime.context 속성의 타입을 안정적으로 보장.\n",
    "    user_id = runtime.context.user_id\n",
    "    # 컨텍스트에서 user_id를 꺼내 비즈니스 로직에 활용.\n",
    "    # 여기선 데모로 user_id == \"1\"이면 Florida, 아니면 SF 반환.\n",
    "    return \"Florida\" if user_id == \"1\" else \"SF\"\n",
    "    # 흐름: 에이전트(혹은 그래프)가 실행시 ToolRuntime(Context(user_id=\"...\"))를 주입\n",
    "    #   -> 툴 내부에서 runtime.context.user_id 사용\n",
    "    #   -> 컨텍스트 기반 결과를 계산해 문자열 반환.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "도구는 잘 문서화되어야 합니다. \n",
    "도구의 이름, 설명, 인수 이름은 모델 프롬프트의 일부가 됩니다. \n",
    "LangChain의 @tool데코레이터는 메타데이터를 추가하고 \n",
    "ToolRuntime매개변수를 통한 런타임 주입을 활성화합니다.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "이 ToolRuntime 클래스는 LangChain/LangGraph에서 툴(tool) 실행 시 \n",
    "필요한 런타임 정보를 하나의 객체로 통합해서 자동으로 넘겨주는 역할을 합니다.\n",
    "툴 함수가 tool_runtime 또는 runtime: ToolRuntime 을 파라미터로 받으면, \n",
    "LangChain 시스템이 내부적으로 상태, 컨텍스트, 실행 설정, 스트림 메시지, 스토리지, 실행 ID 등을 자동 주입합니다.\n",
    "\n",
    "주요 필드 설명\n",
    "state: 현재 그래프의 상태 (예: 대화 메시지, 캐시, 카운터 등 툴 실행 도중 변하는 값 저장)\n",
    "\n",
    "context: 런타임 컨텍스트 (예: 유저 아이디, 세션 정보, 실행 환경 등)\n",
    "\n",
    "config: 개별 실행을 위한 설정 정보 (RunnableConfig), 예를 들어 실행 ID, 리트라이 정책 등\n",
    "\n",
    "stream_writer: 툴 실행 중 실시간으로 메시지나 상태 업데이트를 스트리밍하는 객체\n",
    "\n",
    "tool_call_id: 이번 툴 호출의 고유 ID (트래킹, 로깅 등에 사용)\n",
    "\n",
    "store: 장기적 데이터를 저장하는 객체 (BaseStore), 예를 들어 유저 장기 메모리 등\n",
    "\n",
    "사용 예시\n",
    "툴 함수에서 runtime: ToolRuntime 타입으로 인자를 받으면\n",
    "\n",
    "대화 상태, 스트림 메시지, 사용자 정보 등의 런타임 데이터를 쉽게 사용할 수 있다.\n",
    "\n",
    "코드는 파라미터 이름만 맞추면 되고, 별도 어노테이션이나 래퍼가 필요 없음.\n",
    "\n",
    "즉, 툴 함수 내에서 여러 종류의 런타임 데이터를 쉽게 접근하고 처리할 수 있게 자동화된 편리한 툴 인젝션 구조입니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2d0e9",
   "metadata": {},
   "source": [
    "## 모델 구성\n",
    "- 사용 사례에 맞는 올바른 매개변수로 언어 모델을 설정하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77a312f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ed2545",
   "metadata": {},
   "source": [
    "## 응답 형식 정의\n",
    "- 선택적으로, 에이전트 응답이 특정 스키마와 일치해야 하는 경우 구조화된 응답 형식을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afaf7a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "# 왜: 가벼운 데이터 컨테이너를 쉽게 정의하려고 dataclass 사용.\n",
    "#     필드 타입힌트만 쓰면 __init__/__repr__/__eq__ 등을 자동 생성해 주므로\n",
    "#     \"응답 형태(스키마)\"를 간단히 선언하기에 적합.\n",
    "#     (주석에 적었듯 Pydantic 모델도 지원되므로, 검증이 더 필요하면 Pydantic으로 바꿔도 됨)\n",
    "\n",
    "# We use a dataclass here, but Pydantic models are also supported.\n",
    "@dataclass\n",
    "class ResponseFormat:\n",
    "    \"\"\"Response schema for the agent.\"\"\"\n",
    "    # 왜: 이 클래스는 \"에이전트가 어떤 형식으로 답을 내야 하는가\"를 정의하는 '응답 스키마' 역할.\n",
    "    # 흐름: create_agent(response_format=ResponseFormat) → LLM 출력이 이 구조에 맞추어 생성/파싱.\n",
    "\n",
    "    # A punny response (always required)\n",
    "    punny_response: str\n",
    "    # 무엇: '항상 필요한' 필드(필수). 말장난(pun)이 들어간 한 줄 응답을 문자열로 요구.\n",
    "    # 왜: 모델이 반드시 이 필드를 채우게 해서 프롬프트 의도를 강제하고, \n",
    "    #     후속 코드가 이 값을 신뢰하고 사용 가능(널 체크 없이).\n",
    "\n",
    "    # Any interesting information about the weather if available\n",
    "    weather_conditions: str | None = None\n",
    "    # 무엇: 선택(optional) 필드. 날씨 관련 흥미로운 정보가 있으면 문자열로, 없으면 None.\n",
    "    # 왜: 상황에 따라 정보가 없을 수 있으므로 기본값을 None으로 두어 \"선택 필드\"임을 명확히.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28f0068d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이 스키마가 어디서 어떻게 쓰이나? (흐름)\\n\\n에이전트 생성 시 연결\\n\\ngraph = create_agent(\\n    model=\"anthropic:claude-sonnet-4-5\",\\n    tools=[...],\\n    response_format=ResponseFormat,  # ← 여기!\\n)\\n\\n\\nLangChain이 LLM에게 “응답을 이 구조로 내놔”라고 유도합니다(모델 능력에 따라 JSON/함수호출 등으로).\\n\\n실행 → 구조화 응답 파싱\\n\\n모델이 생성한 출력을 LangChain이 ResponseFormat에 매핑/검증합니다.\\n\\npunny_response는 반드시 채워져야 하고, weather_conditions는 비어 있어도 됩니다.\\n\\n후속 처리 안정성↑\\n\\n이후 코드에서 result.punny_response를 널체크 없이 바로 사용해도 안전.\\n\\nweather_conditions는 if result.weather_conditions: 처럼 선택적으로 활용.\\n\\n왜 dataclass? vs Pydantic?\\n\\ndataclass\\n\\n가볍고 빠르며 표준 라이브러리. “형태 선언” 중심이라 데모/간단 스키마에 적합.\\n\\nPydantic\\n\\n엄격한 유효성 검증, 변환, 에러 메시지가 필요하면 더 적합.\\n\\n예: 날짜/이메일/Enum, 커스텀 검증 로직 등.\\n\\n동일 스키마를 Pydantic으로 쓰면:\\n\\nfrom pydantic import BaseModel, Field\\n\\nclass ResponseFormat(BaseModel):\\n    punny_response: str = Field(..., description=\"말장난 한 줄 응답(필수)\")\\n    weather_conditions: str | None = Field(None, description=\"있으면 날씨 정보\")\\n\\n미니 사용 예 (LangChain 에이전트)\\nfrom langchain.agents import create_agent\\n\\ngraph = create_agent(\\n    model=\"anthropic:claude-sonnet-4-5\",\\n    tools=[],\\n    response_format=ResponseFormat,  # ← 구조화 응답 스키마\\n    system_prompt=\"Always respond with a pun, and include weather info if helpful.\"\\n)\\n\\nout = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"오늘 기분 어때?\"}]})\\n# LangChain이 내부적으로 ResponseFormat에 맞춰 파싱/검증해 반환\\n# (반환 형태는 실행 컨텍스트/버전에 따라 dict 또는 객체일 수 있음) \\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"이 스키마가 어디서 어떻게 쓰이나? (흐름)\n",
    "\n",
    "에이전트 생성 시 연결\n",
    "\n",
    "graph = create_agent(\n",
    "    model=\"anthropic:claude-sonnet-4-5\",\n",
    "    tools=[...],\n",
    "    response_format=ResponseFormat,  # ← 여기!\n",
    ")\n",
    "\n",
    "\n",
    "LangChain이 LLM에게 “응답을 이 구조로 내놔”라고 유도합니다(모델 능력에 따라 JSON/함수호출 등으로).\n",
    "\n",
    "실행 → 구조화 응답 파싱\n",
    "\n",
    "모델이 생성한 출력을 LangChain이 ResponseFormat에 매핑/검증합니다.\n",
    "\n",
    "punny_response는 반드시 채워져야 하고, weather_conditions는 비어 있어도 됩니다.\n",
    "\n",
    "후속 처리 안정성↑\n",
    "\n",
    "이후 코드에서 result.punny_response를 널체크 없이 바로 사용해도 안전.\n",
    "\n",
    "weather_conditions는 if result.weather_conditions: 처럼 선택적으로 활용.\n",
    "\n",
    "왜 dataclass? vs Pydantic?\n",
    "\n",
    "dataclass\n",
    "\n",
    "가볍고 빠르며 표준 라이브러리. “형태 선언” 중심이라 데모/간단 스키마에 적합.\n",
    "\n",
    "Pydantic\n",
    "\n",
    "엄격한 유효성 검증, 변환, 에러 메시지가 필요하면 더 적합.\n",
    "\n",
    "예: 날짜/이메일/Enum, 커스텀 검증 로직 등.\n",
    "\n",
    "동일 스키마를 Pydantic으로 쓰면:\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    punny_response: str = Field(..., description=\"말장난 한 줄 응답(필수)\")\n",
    "    weather_conditions: str | None = Field(None, description=\"있으면 날씨 정보\")\n",
    "\n",
    "미니 사용 예 (LangChain 에이전트)\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "graph = create_agent(\n",
    "    model=\"anthropic:claude-sonnet-4-5\",\n",
    "    tools=[],\n",
    "    response_format=ResponseFormat,  # ← 구조화 응답 스키마\n",
    "    system_prompt=\"Always respond with a pun, and include weather info if helpful.\"\n",
    ")\n",
    "\n",
    "out = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"오늘 기분 어때?\"}]})\n",
    "# LangChain이 내부적으로 ResponseFormat에 맞춰 파싱/검증해 반환\n",
    "# (반환 형태는 실행 컨텍스트/버전에 따라 dict 또는 객체일 수 있음) \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e019b274",
   "metadata": {},
   "source": [
    "## 메모리 추가\n",
    "- 에이전트에 **메모리**를 추가하여 상호작용 전반의 상태를 유지하세요\n",
    "- 이를 통해 에이전트는 이전 대화와 맥락을 기억할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97403f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "# 무엇: LangGraph의 \"체크포인트 저장소\" 구현 중 하나를 임포트.\n",
    "# 왜: 그래프 실행 중간중간의 상태(state)를 메모리에 저장해서\n",
    "#     재시작(재진입)하거나, 스트리밍 중간 결과를 이어받을 수 있게 하기 위해.\n",
    "#     (= 대화 메모리/히스토리, interrupt 후 재개 등을 가능하게 하는 핵심 컴포넌트)\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "# 무엇: 프로세스 메모리에 상태를 저장하는 체크포인터 인스턴스 생성.\n",
    "# 왜: 빠르고 설정이 간단함. 데모/개발/단일 프로세스 환경에서 유용.\n",
    "#    단, 프로세스가 내려가면(서버 재시작) 저장 내용이 사라짐 → 영속성은 없음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40209cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n체크포인터가 정확히 뭔가요?\\n\\nLangGraph는 노드 간 **상태(State)**를 전달하며 실행됩니다.\\n\\n체크포인터는 이 상태 스냅샷을 특정 타이밍(노드 끝, interrupt 시점 등)에 저장해 둡니다.\\n\\n이렇게 저장된 상태는 나중에 같은 thread_id(대화 세션/작업 ID)로 다시 불러와 재개할 수 있어요.\\n\\n예) interrupt()로 사람이 승인할 때까지 멈춤 → 승인이 오면 그 시점 상태에서 Command(resume=...)로 이어가기\\n\\nInMemorySaver의 특징 (장단점)\\n\\n✅ 장점\\n\\n설정이 제일 간단하고 빠름(로컬 메모리 dict 수준).\\n\\n테스트/개발/단일 인스턴스 PoC에 최적.\\n\\n⚠️ 단점\\n\\n프로세스 재기동/크래시 시 데이터 소실.\\n\\n여러 서버(멀티 인스턴스)에서 공유 불가.\\n\\n상태가 커지면 메모리 사용량 증가 → 관리 필요.\\n\\n운영/배포에서는 보통 SqliteSaver, PostgresSaver 같은 영속 저장소를 써요. 그래야 서버 재시작이나 스케일아웃에서도 대화/작업 상태가 유지됩니다.\\n\\n어떻게 연결하나요? (미니 예시)\\nfrom typing import TypedDict, Annotated\\nfrom langgraph.graph import StateGraph, add_messages, END\\nfrom langgraph.checkpoint.memory import InMemorySaver\\nfrom langgraph.types import interrupt, Command\\n\\n# 1) 상태 스키마 정의\\nclass S(TypedDict):\\n    messages: Annotated[list, add_messages]\\n    approved: bool | None\\n\\n# 2) 노드 정의\\ndef draft(state: S):\\n    return {\"messages\": [{\"role\":\"ai\",\"content\":\"초안 v1\"}]}\\n\\ndef review(state: S):\\n    # 중요 지점: 사람 승인을 받아야 하므로 여기서 중단\\n    _ = interrupt({\"reason\":\"need_approval\", \"draft\":\"초안 v1\"})\\n    # resume 값이 들어오면 계속 실행\\n    return {}\\n\\ndef maybe_end(state: S):\\n    return END\\n\\n# 3) 그래프 구성\\nbuilder = StateGraph(S)\\nbuilder.add_node(\"draft\", draft)\\nbuilder.add_node(\"review\", review)\\nbuilder.add_node(\"maybe_end\", maybe_end)\\nbuilder.add_edge(\"draft\", \"review\")\\nbuilder.add_edge(\"review\", \"maybe_end\")\\nbuilder.set_entry_point(\"draft\")\\n\\n# 4) 체크포인터 주입 (여기가 핵심)\\ncheckpointer = InMemorySaver()\\ngraph = builder.compile(checkpointer=checkpointer)\\n\\n# 5) 실행 (thread_id는 같은 대화/작업을 식별)\\nthread = {\"configurable\": {\"thread_id\": \"brand-001\"}}\\n\\n# 첫 실행: review에서 interrupt로 멈춤 → UI/서버가 payload를 수신\\nevents = graph.stream({\"messages\":[{\"role\":\"user\",\"content\":\"브리프 만들어줘\"}]}, thread)\\nfor ev in events:\\n    print(ev)  # 여기서 interrupt 신호를 받음\\n\\n# 사용자가 승인했다고 가정 → 재개\\ngraph.invoke(Command(resume={\"approve\": True}), thread)\\n\\n흐름 정리\\n\\ncompile(checkpointer=InMemorySaver())로 그래프가 상태 저장 지원을 갖추게 됩니다.\\n\\nstream/invoke 호출 시 thread_id로 세션을 식별해 같은 대화/작업의 상태를 찾아 씁니다.\\n\\ninterrupt()를 만나면 상태를 저장하고 정지 → 외부에서 승인/수정값을 받아 Command(resume=...)로 같은 지점부터 계속.\\n\\n언제 InMemorySaver를 쓸까?\\n\\n튜토리얼/데모/개발 단계\\n\\n단일 프로세스(로컬, 노트북, 한 개의 uvicorn worker)에서 빠르게 테스트\\n\\n상태 유실되어도 괜찮은 임시 워크플로\\n\\n언제 다른 Saver가 필요할까?\\n\\n서버 재시작 후에도 세션을 이어가야 함\\n\\n여러 인스턴스(오토스케일)에서 같은 세션을 이어가야 함\\n\\n대용량 상태(리서치 캐시, 검색 결과 등)를 안정적으로 관리해야 함\\n\\n이럴 때는 파일/DB 기반 체크포인터를 고려하세요(예: SQLite, Postgres).\\n또한 LangGraph의 store(전역 데이터 저장)와 함께 쓰면 여러 스레드/유저 간 공유 데이터도 다룰 수 있습니다.\\n\\n한 줄 요약\\n\\nInMemorySaver()는 그래프 상태를 RAM에 저장하는 초간단 체크포인터예요.\\n빠른 개발/테스트용으로 최고지만, 영속성/확장성이 필요하면 DB 기반 Saver로 바꾸세요.\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "체크포인터가 정확히 뭔가요?\n",
    "\n",
    "LangGraph는 노드 간 **상태(State)**를 전달하며 실행됩니다.\n",
    "\n",
    "체크포인터는 이 상태 스냅샷을 특정 타이밍(노드 끝, interrupt 시점 등)에 저장해 둡니다.\n",
    "\n",
    "이렇게 저장된 상태는 나중에 같은 thread_id(대화 세션/작업 ID)로 다시 불러와 재개할 수 있어요.\n",
    "\n",
    "예) interrupt()로 사람이 승인할 때까지 멈춤 → 승인이 오면 그 시점 상태에서 Command(resume=...)로 이어가기\n",
    "\n",
    "InMemorySaver의 특징 (장단점)\n",
    "\n",
    "✅ 장점\n",
    "\n",
    "설정이 제일 간단하고 빠름(로컬 메모리 dict 수준).\n",
    "\n",
    "테스트/개발/단일 인스턴스 PoC에 최적.\n",
    "\n",
    "⚠️ 단점\n",
    "\n",
    "프로세스 재기동/크래시 시 데이터 소실.\n",
    "\n",
    "여러 서버(멀티 인스턴스)에서 공유 불가.\n",
    "\n",
    "상태가 커지면 메모리 사용량 증가 → 관리 필요.\n",
    "\n",
    "운영/배포에서는 보통 SqliteSaver, PostgresSaver 같은 영속 저장소를 써요. 그래야 서버 재시작이나 스케일아웃에서도 대화/작업 상태가 유지됩니다.\n",
    "\n",
    "어떻게 연결하나요? (미니 예시)\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, add_messages, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "# 1) 상태 스키마 정의\n",
    "class S(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    approved: bool | None\n",
    "\n",
    "# 2) 노드 정의\n",
    "def draft(state: S):\n",
    "    return {\"messages\": [{\"role\":\"ai\",\"content\":\"초안 v1\"}]}\n",
    "\n",
    "def review(state: S):\n",
    "    # 중요 지점: 사람 승인을 받아야 하므로 여기서 중단\n",
    "    _ = interrupt({\"reason\":\"need_approval\", \"draft\":\"초안 v1\"})\n",
    "    # resume 값이 들어오면 계속 실행\n",
    "    return {}\n",
    "\n",
    "def maybe_end(state: S):\n",
    "    return END\n",
    "\n",
    "# 3) 그래프 구성\n",
    "builder = StateGraph(S)\n",
    "builder.add_node(\"draft\", draft)\n",
    "builder.add_node(\"review\", review)\n",
    "builder.add_node(\"maybe_end\", maybe_end)\n",
    "builder.add_edge(\"draft\", \"review\")\n",
    "builder.add_edge(\"review\", \"maybe_end\")\n",
    "builder.set_entry_point(\"draft\")\n",
    "\n",
    "# 4) 체크포인터 주입 (여기가 핵심)\n",
    "checkpointer = InMemorySaver()\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# 5) 실행 (thread_id는 같은 대화/작업을 식별)\n",
    "thread = {\"configurable\": {\"thread_id\": \"brand-001\"}}\n",
    "\n",
    "# 첫 실행: review에서 interrupt로 멈춤 → UI/서버가 payload를 수신\n",
    "events = graph.stream({\"messages\":[{\"role\":\"user\",\"content\":\"브리프 만들어줘\"}]}, thread)\n",
    "for ev in events:\n",
    "    print(ev)  # 여기서 interrupt 신호를 받음\n",
    "\n",
    "# 사용자가 승인했다고 가정 → 재개\n",
    "graph.invoke(Command(resume={\"approve\": True}), thread)\n",
    "\n",
    "흐름 정리\n",
    "\n",
    "compile(checkpointer=InMemorySaver())로 그래프가 상태 저장 지원을 갖추게 됩니다.\n",
    "\n",
    "stream/invoke 호출 시 thread_id로 세션을 식별해 같은 대화/작업의 상태를 찾아 씁니다.\n",
    "\n",
    "interrupt()를 만나면 상태를 저장하고 정지 → 외부에서 승인/수정값을 받아 Command(resume=...)로 같은 지점부터 계속.\n",
    "\n",
    "언제 InMemorySaver를 쓸까?\n",
    "\n",
    "튜토리얼/데모/개발 단계\n",
    "\n",
    "단일 프로세스(로컬, 노트북, 한 개의 uvicorn worker)에서 빠르게 테스트\n",
    "\n",
    "상태 유실되어도 괜찮은 임시 워크플로\n",
    "\n",
    "언제 다른 Saver가 필요할까?\n",
    "\n",
    "서버 재시작 후에도 세션을 이어가야 함\n",
    "\n",
    "여러 인스턴스(오토스케일)에서 같은 세션을 이어가야 함\n",
    "\n",
    "대용량 상태(리서치 캐시, 검색 결과 등)를 안정적으로 관리해야 함\n",
    "\n",
    "이럴 때는 파일/DB 기반 체크포인터를 고려하세요(예: SQLite, Postgres).\n",
    "또한 LangGraph의 store(전역 데이터 저장)와 함께 쓰면 여러 스레드/유저 간 공유 데이터도 다룰 수 있습니다.\n",
    "\n",
    "한 줄 요약\n",
    "\n",
    "InMemorySaver()는 그래프 상태를 RAM에 저장하는 초간단 체크포인터예요.\n",
    "빠른 개발/테스트용으로 최고지만, 영속성/확장성이 필요하면 DB 기반 Saver로 바꾸세요.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df83978",
   "metadata": {},
   "source": [
    "## 에이전트를 생성하고 실행합니다.\n",
    "- 모든 구성 요소를 사용해 에이전트를 조립하고 실행해 보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0475994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponseFormat(punny_response='플로리다의 날씨는 항상 햇살이 가득해요! 햇살이 너무 좋으니, \"햇살이 나를 부른다\"고 할 수 있겠네요!', weather_conditions=\"It's always sunny in Florida!\")\n",
      "ResponseFormat(punny_response='별 말씀을요! 언제든지 날씨에 대해 궁금한 점이 있으면 \"햇살처럼\" 찾아와 주세요!', weather_conditions=None)\n",
      "<class 'dict'>\n",
      "dict_keys(['messages', 'structured_response'])\n",
      "<class 'langchain_core.messages.tool.ToolMessage'> {}\n",
      "Returning structured response: ResponseFormat(punny_response='별 말씀을요! 언제든지 날씨에 대해 궁금한 점이 있으면 \"햇살처럼\" 찾아와 주세요!', weather_conditions=None)\n"
     ]
    }
   ],
   "source": [
    "# 에이전트(그래프) 생성\n",
    "agent = create_agent(\n",
    "    model=model,                           # 무엇: 사용할 LLM (예: ChatOpenAI/Anthropic 등 인스턴스 또는 문자열 식별자)\n",
    "    system_prompt=SYSTEM_PROMPT,           # 왜: 대화 전반의 행동 정책/톤을 지정(시스템 메세지로 선반영)\n",
    "    tools=[get_user_location, get_weather_for_location],  # 무엇: LLM이 호출할 수 있는 도구 목록(위에서 @tool로 등록)\n",
    "    context_schema=Context,                # 왜: ToolRuntime에 주입되는 \"런타임 컨텍스트\"의 스키마(데이터 형태)를 명시\n",
    "    response_format=ResponseFormat,        # 왜: 에이전트의 \"구조화 응답 스키마\" 지정(필수/옵션 필드, 타입)\n",
    "    checkpointer=checkpointer              # 왜: 대화 상태를 저장/복구(메모리 또는 DB). 같은 thread_id로 이어붙이기 가능\n",
    ")\n",
    "\n",
    "# thread_id: 한 \"대화/세션\"을 식별하는 고유 값\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "# 왜: 동일한 thread_id를 쓰면, 같은 대화 히스토리/상태가 이어짐(메모리/체크포인터 이용)\n",
    "\n",
    "# --- 1차 호출: \"밖 날씨 어때?\" ---\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]},\n",
    "    config=config,                # 무엇: 위에서 정의한 thread_id 설정(상태 이어받기)\n",
    "    context=Context(user_id=\"1\")  # 흐름: 실행 시 ToolRuntime에 주입될 컨텍스트(여기선 user_id만 포함)\n",
    ")\n",
    "\n",
    "# 흐름 요약:\n",
    "#  1) LLM이 메시지/시스템 프롬프트를 보고 목적 판단\n",
    "#  2) 도구 호출 필요 판단 → get_user_location(runtime) 호출(컨텍스트로 user_id=\"1\")\n",
    "#  3) 위치 \"Florida\" 반환 → 이어서 get_weather_for_location(city=\"Florida\") 호출\n",
    "#  4) 두 툴 결과를 바탕으로 LLM이 ResponseFormat 스키마에 맞게 structured_response 생성\n",
    "\n",
    "print(response['structured_response'])\n",
    "# 무엇: LangChain이 response_format에 맞춰 파싱한 \"구조화 응답\"\n",
    "# 형태: ResponseFormat(punny_response=..., weather_conditions=...)\n",
    "# 장점: 이후 로직에서 키 안전하게 접근(널/형 변환 오류 감소)\n",
    "\n",
    "\n",
    "# --- 같은 thread_id로 대화를 계속 ---\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]},\n",
    "    config=config,                # 동일 thread_id → 같은 히스토리/상태를 활용\n",
    "    context=Context(user_id=\"1\")  # 컨텍스트 계속 주입(필요 시 사용자/권한/테넌트 등 바꿔도 됨)\n",
    ")\n",
    "# 흐름:\n",
    "#  - 이전 턴에서 쌓인 메시지/도구 결과를 기억한 채(체크포인터) 응답\n",
    "#  - 이번에는 날씨 도구 호출이 필요 없으니 punny_response만 채우고 weather_conditions는 None일 수 있음\n",
    "\n",
    "print(response['structured_response'])\n",
    "\n",
    "print(type(response))\n",
    "print(response.keys())\n",
    "\n",
    "msgs = response.get(\"messages\", [])\n",
    "last = msgs[-1] if msgs else None\n",
    "if last:\n",
    "    print(type(last), getattr(last, \"additional_kwargs\", {}))\n",
    "    print(getattr(last, \"content\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324265fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "첫 번째 응답\n",
    "punny_response:\n",
    "\"플로리다는 오늘도 '해가 쨍쨍-더풀(sun-derful)' 하네요! 햇살이 온종일 '레이디오(ray-dio)' 히트곡을 틀어주고 있어요! '태양 축제(solar-bration)' 하기 딱 좋은 날씨죠! 비를 원하셨다면… 그 생각은 '물거품(washed up)'이 될 듯—예보는 여전히 '쾌-청(clear-ly)'합니다!\"\n",
    "\n",
    "weather_conditions:\n",
    "\"플로리다는 언제나 맑아요!\"\n",
    "\n",
    "두 번째 응답\n",
    "punny_response:\n",
    "\"별 말씀을요—'천둥(thund-)처럼' 환영합니다! 언제든지 날씨 '최신(current)' 소식으로 도와드릴게요. 저는 그냥 '구름(cloud)'처럼 떠다니며 여러분께 '소나기(shower)' 같은 정보 뿌릴 준비가 되어 있답니다. 플로리다의 '해-쨍(sun-sational)'한 하루 되세요!\"\n",
    "\n",
    "weather_conditions:\n",
    "None\n",
    "\n",
    "한 줄 요약\n",
    "\n",
    "이 스니펫은 컨텍스트 주입 + 도구 호출 + 구조화 응답 + 상태 지속(thread_id) 를 한 번에 보여주는 LangGraph/Agent 표준 패턴입니다.\n",
    "\n",
    "같은 thread_id로 이어가면 대화 맥락이 유지되고, response_format 덕분에 출력 이용이 안정적입니다.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "정리: 'structured_response'는 어디서 생기나?\n",
    "\n",
    "왜 생기나\n",
    "response_format=ResponseFormat를 주면, 내부에서 구조화 출력 미들웨어/전략이 작동해 LLM의 답을 ResponseFormat 스키마에 맞춰 파싱합니다.\n",
    "이때 **상태 딕셔너리(state)**에 파싱 결과를 **structured_response**라는 키로 추가하는 구현(최근 LangChain/agents 기준)이 있습니다.\n",
    "\n",
    "왜 안 보일 수 있나 (자주 겪는 케이스)\n",
    "\n",
    "버전 차이: 어떤 버전에선 키 이름이 다르거나(예: response, final_response) 구조화 출력이 메시지에만 붙습니다.\n",
    "\n",
    "모델/전략 미스매치: 사용 모델이 구조화 출력을 제대로 지원하지 않으면 파싱이 생략되어 키가 생성되지 않음.\n",
    "\n",
    "실행 모드: stream()의 stream_mode에 따라 업데이트 조각만 오고 최종 병합 상태가 아니라서 키가 없음.\n",
    "\n",
    "도큐스트링/스키마 불일치: 스키마와 프롬프트 규칙이 어긋나면 파서가 실패 → 키 미생성.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f05345",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "256b96ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add 3 and 4.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  add (call_Y2glfrli3aDPcJeebyyqUbCD)\n",
      " Call ID: call_Y2glfrli3aDPcJeebyyqUbCD\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 4\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "7\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "3 + 4 = 7.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define tools and model\n",
    "\n",
    "from langchain.tools import tool\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"gpt-5-nano\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Define tools\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply `a` and `b`.\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds `a` and `b`.\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide `a` and `b`.\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "# Augment the LLM with tools\n",
    "tools = [add, multiply, divide]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "# Step 2: Define state\n",
    "from langchain.messages import AnyMessage\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    llm_calls: int\n",
    "\n",
    "# Step 3: Define model node\n",
    "from langchain.messages import SystemMessage\n",
    "def llm_call(state: dict):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            model_with_tools.invoke(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "                    )\n",
    "                ]\n",
    "                + state[\"messages\"]\n",
    "            )\n",
    "        ],\n",
    "        \"llm_calls\": state.get('llm_calls', 0) + 1\n",
    "    }\n",
    "\n",
    "# Step 4: Define tool node\n",
    "\n",
    "from langchain.messages import ToolMessage\n",
    "\n",
    "def tool_node(state: dict):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": result}\n",
    "\n",
    "# Step 5: Define logic to determine whether to end\n",
    "from typing import Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n",
    "def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n",
    "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "   \n",
    "    # If the LLM makes a tool call, then perform an action\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node\"\n",
    "\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "# Step 6: Build agent\n",
    "# Build workflow\n",
    "agent_builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    [\"tool_node\", END]\n",
    ")\n",
    "agent_builder.add_edge(\"tool_node\", \"llm_call\")\n",
    "\n",
    "# Compile the agent\n",
    "agent = agent_builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "# Show the agent\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n",
    "\n",
    "# Invoke\n",
    "from langchain.messages import HumanMessage\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "messages = agent.invoke({\"messages\": messages})\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bfd1db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total LLM calls: 2\n"
     ]
    }
   ],
   "source": [
    "llm_calls = messages[\"llm_calls\"]\n",
    "print(f\"Total LLM calls: {llm_calls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6149eeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Add 3 and 4.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 154, 'prompt_tokens': 246, 'total_tokens': 400, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CdS62wIgayAP5nvqUJHthn6kiL8uo', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--7c7ad9ec-5274-4a80-b9d5-08f4b19701de-0', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 4}, 'id': 'call_N9MhAVOHGD2RUt9Gwx4Tzu5N', 'type': 'tool_call'}], usage_metadata={'input_tokens': 246, 'output_tokens': 154, 'total_tokens': 400, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}),\n",
       "  ToolMessage(content='7', tool_call_id='call_N9MhAVOHGD2RUt9Gwx4Tzu5N'),\n",
       "  AIMessage(content='The sum of 3 and 4 is 7.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 213, 'prompt_tokens': 277, 'total_tokens': 490, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CdS65Ev15UgqyIdL7x644kmFqSMqz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--5b5a1007-fe27-487f-a117-580edae1b094-0', usage_metadata={'input_tokens': 277, 'output_tokens': 213, 'total_tokens': 490, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}})],\n",
       " 'llm_calls': 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d83e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Literal, Dict, Any, Optional\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.types import interrupt\n",
    "from langchain.agents import AgentState\n",
    "\n",
    "# ===== 1. 상태 정의 =====\n",
    "\n",
    "class BrandProfile(TypedDict, total=False):\n",
    "    brand_name: str\n",
    "    category: str\n",
    "    tone_mood: str\n",
    "    core_keywords: str\n",
    "    slogan: str\n",
    "    target_age: str\n",
    "    target_gender: str\n",
    "    avoided_trends: str\n",
    "    preferred_colors: str\n",
    "\n",
    "class LogoInfo(TypedDict, total=False):\n",
    "    logo_id: int\n",
    "    file_path: str\n",
    "\n",
    "class TrendContext(TypedDict, total=False):\n",
    "    \"\"\"\n",
    "    트렌드 분석 관련해서 공통으로 쓰일 컨텍스트/캐시.\n",
    "    - 각 에이전트(브랜드/로고/숏폼)가 트렌드를 호출할 때\n",
    "      동일한 질의를 반복하지 않도록 캐시 역할도 할 수 있음.\n",
    "    \"\"\"\n",
    "    last_query: Optional[str]      # 마지막으로 분석했던 질의 설명\n",
    "    last_result_summary: Optional[str]  # 요약 결과(LLM가 만든 자연어)\n",
    "\n",
    "class ShortformState(AgentState):\n",
    "\n",
    "    brand_profile: BrandProfile\n",
    "    user_request: str | None\n",
    "    # use_trend_research: bool | None\n",
    "\n",
    "    trend_context: TrendContext\n",
    "    \n",
    "    has_logo: bool | None\n",
    "    logo_candidates: list[LogoInfo] | None\n",
    "    selected_logo: LogoInfo | None\n",
    "\n",
    "    veo_prompt: str | None\n",
    "    local_video_path: str | None\n",
    "    save_decision: Literal[\"Y\", \"N\"] | None\n",
    "\n",
    "# ===== 2. 외부 의존성 (LLM, Veo, DB) — 예시 =====\n",
    "\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import time\n",
    "# import cx_Oracle  # Oracle 예시\n",
    "\n",
    "# veo_client = genai.Client()  # 실제로는 api_key, project 등 설정\n",
    "DB_DSN = \"user/pw@localhost:1521/XEPDB1\"  # 예시 DSN\n",
    "\n",
    "def get_db_connection():\n",
    "    # 왜 함수로 뺐나? → 노드에서 DB 커넥션 로직을 중복하지 않기 위해.\n",
    "    return cx_Oracle.connect(DB_DSN)\n",
    "\n",
    "\n",
    "# trend_agent도 이미 있다고 했으니, 여기서는 래퍼만 정의\n",
    "def run_trend_agent_for_brand(brand: BrandProfile, extra_prompt: str | None) -> str:\n",
    "    \"\"\"\n",
    "    왜 별도 함수?\n",
    "    - LangGraph 노드는 state만 알고 있어야하고, 실제 trend_agent 호출 세부 로직은\n",
    "      함수로 분리해두면 테스트/교체가 쉬움.\n",
    "    \"\"\"\n",
    "    # TODO: 실제 trend_agent.invoke(...)로 교체\n",
    "    return \"2025년 숏폼 트렌드 요약 텍스트 (예시)\"\n",
    "\n",
    "\n",
    "# ===== 3. 노드 함수들 =====\n",
    "\n",
    "def init_shortform(state: ShortformState) -> Dict:\n",
    "    \"\"\"\n",
    "    시작 노드:\n",
    "    - 브랜드 프로필이 있는지 확인\n",
    "    - extra_prompt / use_trend_research를 기본값으로 보정\n",
    "    \"\"\"\n",
    "    if not state.get(\"brand_profile\"):\n",
    "        return {\"error\": \"브랜드 정보가 없습니다. 먼저 Phase1을 완료해주세요.\"}\n",
    "    return {\n",
    "        \"extra_prompt\": state.get(\"extra_prompt\") or \"\",\n",
    "        \"use_trend_research\": bool(state.get(\"use_trend_research\"))\n",
    "    }\n",
    "\n",
    "\n",
    "def node_trend_agent(state: ShortformState) -> Dict:\n",
    "    \"\"\"\n",
    "    선택 노드:\n",
    "    - use_trend_research == True일 때만 호출\n",
    "    - trend_summary에 결과 저장\n",
    "    \"\"\"\n",
    "    if not state.get(\"use_trend_research\"):\n",
    "        return {}  # 아무것도 안 바꿈\n",
    "\n",
    "    summary = run_trend_agent_for_brand(\n",
    "        state[\"brand_profile\"],\n",
    "        state.get(\"extra_prompt\") or \"\"\n",
    "    )\n",
    "    return {\"trend_summary\": summary}\n",
    "\n",
    "\n",
    "def node_check_logo(state: ShortformState) -> Dict:\n",
    "    \"\"\"\n",
    "    DB에서 로고 유무 확인.\n",
    "    왜 코드를 쓰고 LLM을 안쓰나?\n",
    "    - 로고 존재 여부는 '사실'이라 LLM 추론이 아니라 DB 질의가 맞음.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = get_db_connection()\n",
    "        cur = conn.cursor()\n",
    "        # 예시: BRAND_LOGO 테이블에서 조회\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT logo_id, file_path\n",
    "            FROM BRAND_LOGO\n",
    "            WHERE brand_name = :brand_name\n",
    "        \"\"\", brand_name=state[\"brand_profile\"][\"brand_name\"])\n",
    "        rows = cur.fetchall()\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"DB 오류: {e}\", \"has_logo\": False}\n",
    "\n",
    "    if not rows:\n",
    "        return {\n",
    "            \"has_logo\": False,\n",
    "            \"logo_candidates\": []\n",
    "        }\n",
    "\n",
    "    logos: list[LogoInfo] = [\n",
    "        {\"logo_id\": r[0], \"file_path\": r[1]} for r in rows\n",
    "    ]\n",
    "    return {\n",
    "        \"has_logo\": True,\n",
    "        \"logo_candidates\": logos\n",
    "    }\n",
    "\n",
    "\n",
    "def node_select_logo(state: ShortformState) -> Dict:\n",
    "    \"\"\"\n",
    "    로고 선택 노드 (human-in-the-loop).\n",
    "    - LangGraph interrupt() 사용해서 사용자 선택을 기다린다.\n",
    "    \"\"\"\n",
    "    if not state.get(\"has_logo\"):\n",
    "        return {}  # 로고 없으면 아무것도 안함\n",
    "\n",
    "    # 유저에게 보여줄 정보 (프론트/챗 UI에서 활용)\n",
    "    selection = interrupt({\n",
    "        \"type\": \"select_logo\",\n",
    "        \"candidates\": state.get(\"logo_candidates\", [])\n",
    "    })\n",
    "    # selection 예: {\"logo_id\": 12}\n",
    "    logo_id = selection[\"logo_id\"]\n",
    "    selected = next(\n",
    "        (l for l in state.get(\"logo_candidates\", []) if l[\"logo_id\"] == logo_id),\n",
    "        None\n",
    "    )\n",
    "    return {\"selected_logo\": selected}\n",
    "\n",
    "\n",
    "# LLM은 네가 쓰는 OpenAI/Gemini 아무거나 여기서 호출\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm = init_chat_model(\"gpt-4.1-mini\", temperature=0.7)\n",
    "\n",
    "def node_build_prompt(state: ShortformState) -> Dict:\n",
    "    \"\"\"\n",
    "    Veo용 프롬프트 생성 노드.\n",
    "    - 브랜드 정보 + (옵션) 트렌드 요약 + extra_prompt + 로고 여부를 종합.\n",
    "    \"\"\"\n",
    "    brand = state[\"brand_profile\"]\n",
    "    extra = state.get(\"extra_prompt\") or \"\"\n",
    "    trend = state.get(\"trend_summary\") or \"별도의 트렌드 요약 없음\"\n",
    "\n",
    "    has_logo = bool(state.get(\"has_logo\"))\n",
    "    logo_hint = \"A static logo appears at the end card in the bottom-right corner.\" if has_logo else \\\n",
    "                \"The brand logo style should be softly integrated into the scenes without any existing logo file.\"\n",
    "\n",
    "    prompt_template = f\"\"\"\n",
    "You are writing a cinematic 8-second 9:16 short-form video prompt for Google Veo 3.1.\n",
    "\n",
    "[Brand Info]\n",
    "- Name: {brand.get(\"brand_name\")}\n",
    "- Industry: {brand.get(\"category\")}\n",
    "- Mood: {brand.get(\"tone_mood\")}\n",
    "- Core Keywords: {brand.get(\"core_keywords\")}\n",
    "- Slogan: {brand.get(\"slogan\")}\n",
    "- Target: {brand.get(\"target_age\")} {brand.get(\"target_gender\")}\n",
    "- Avoided Trends: {brand.get(\"avoided_trends\")}\n",
    "- Preferred Colors: {brand.get(\"preferred_colors\")}\n",
    "\n",
    "[Trend Summary]\n",
    "{trend}\n",
    "\n",
    "[Additional User Request]\n",
    "{extra}\n",
    "\n",
    "[Logo Usage]\n",
    "{logo_hint}\n",
    "\n",
    "Write a single, coherent English prompt optimized for Veo 3.1.\n",
    "Include 3-4 ultra-short shots, clear camera movement, and simple Korean on-screen text only when needed.\n",
    "    \"\"\".strip()\n",
    "\n",
    "    msg = llm.invoke([HumanMessage(content=prompt_template)])\n",
    "    return {\"veo_prompt\": msg.content}\n",
    "\n",
    "\n",
    "def node_generate_video(state: ShortformState) -> Dict:\n",
    "    \"\"\"\n",
    "    Veo 3.1 호출 노드.\n",
    "    - has_logo에 따라 image + last_frame 설정 여부가 달라짐.\n",
    "    - 실제 API 에러나 대기 시간은 여기서 처리.\n",
    "    \"\"\"\n",
    "    prompt = state[\"veo_prompt\"]\n",
    "    if not prompt:\n",
    "        return {\"error\": \"veo_prompt가 비어 있습니다.\"}\n",
    "\n",
    "    has_logo = bool(state.get(\"has_logo\"))\n",
    "    config = types.GenerateVideosConfig(\n",
    "        duration_seconds=8,\n",
    "        aspect_ratio=\"9:16\",\n",
    "        resolution=\"720p\"\n",
    "    )\n",
    "\n",
    "    if has_logo and state.get(\"selected_logo\"):\n",
    "        logo_path = state[\"selected_logo\"][\"file_path\"]\n",
    "        logo_image = types.Image.from_file(location=logo_path)\n",
    "        operation = veo_client.models.generate_videos(\n",
    "            model=\"veo-3.1-fast-generate-preview\",\n",
    "            prompt=prompt,\n",
    "            image=logo_image,\n",
    "            config=config,\n",
    "        )\n",
    "    else:\n",
    "        operation = veo_client.models.generate_videos(\n",
    "            model=\"veo-3.1-fast-generate-preview\",\n",
    "            prompt=prompt,\n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "    # 폴링 (간단 예시)\n",
    "    while not operation.done:\n",
    "        time.sleep(10)\n",
    "        operation = veo_client.operations.get(operation)\n",
    "\n",
    "    video = operation.response.generated_videos[0]\n",
    "    veo_client.files.download(file=video.video)\n",
    "    save_path = \"outputs/shortform_video.mp4\"\n",
    "    video.video.save(save_path)\n",
    "\n",
    "    return {\"local_video_path\": save_path}\n",
    "\n",
    "\n",
    "def node_confirm_save(state: ShortformState) -> Dict:\n",
    "    \"\"\"\n",
    "    저장 여부를 물어보는 인터럽트 노드.\n",
    "    - 챗 UI에는 \"영상 생성 완료! DB에 저장할까요? (Y/N)\" 같은 메시지를 표시.\n",
    "    \"\"\"\n",
    "    if not state.get(\"local_video_path\"):\n",
    "        return {\"error\": \"저장할 비디오가 없습니다.\"}\n",
    "\n",
    "    decision = interrupt({\n",
    "        \"type\": \"confirm_save\",\n",
    "        \"message\": \"영상 생성이 완료되었습니다. DB에 파일 경로를 저장할까요? (Y/N)\"\n",
    "    })\n",
    "\n",
    "    # decision 예: {\"answer\": \"Y\"} or {\"answer\": \"N\"}\n",
    "    ans = decision.get(\"answer\", \"N\").upper()\n",
    "    return {\"save_decision\": \"Y\" if ans == \"Y\" else \"N\"}\n",
    "\n",
    "\n",
    "def node_save_to_db(state: ShortformState) -> Dict:\n",
    "    \"\"\"\n",
    "    저장 노드.\n",
    "    - save_decision == \"Y\"인 경우에만 Oracle DB에 메타데이터 저장.\n",
    "    \"\"\"\n",
    "    if state.get(\"save_decision\") != \"Y\":\n",
    "        return {}\n",
    "\n",
    "    path = state.get(\"local_video_path\")\n",
    "    if not path:\n",
    "        return {\"error\": \"파일 경로가 없습니다.\"}\n",
    "\n",
    "    try:\n",
    "        conn = get_db_connection()\n",
    "        cur = conn.cursor()\n",
    "        # 예: SHORTFORM_VIDEO 테이블\n",
    "        cur.execute(\"\"\"\n",
    "            INSERT INTO SHORTFORM_VIDEO (\n",
    "                brand_name,\n",
    "                video_path,\n",
    "                has_logo,\n",
    "                created_at\n",
    "            ) VALUES (\n",
    "                :brand_name,\n",
    "                :video_path,\n",
    "                :has_logo,\n",
    "                SYSTIMESTAMP\n",
    "            )\n",
    "        \"\"\", brand_name=state[\"brand_profile\"][\"brand_name\"],\n",
    "             video_path=path,\n",
    "             has_logo=1 if state.get(\"has_logo\") else 0)\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"DB 저장 실패: {e}\"}\n",
    "\n",
    "    return {}  # 특별히 추가 상태는 없음\n",
    "\n",
    "\n",
    "# ===== 4. 그래프 빌드 =====\n",
    "\n",
    "builder = StateGraph(ShortformState)\n",
    "\n",
    "builder.add_node(\"init_shortform\", init_shortform)\n",
    "builder.add_node(\"trend_agent\", node_trend_agent)\n",
    "builder.add_node(\"check_logo\", node_check_logo)\n",
    "builder.add_node(\"select_logo\", node_select_logo)\n",
    "builder.add_node(\"build_prompt\", node_build_prompt)\n",
    "builder.add_node(\"generate_video\", node_generate_video)\n",
    "builder.add_node(\"confirm_save\", node_confirm_save)\n",
    "builder.add_node(\"save_to_db\", node_save_to_db)\n",
    "\n",
    "# --- 엣지 설정 ---\n",
    "\n",
    "builder.add_edge(START, \"init_shortform\")\n",
    "builder.add_edge(\"init_shortform\", \"trend_agent\")\n",
    "builder.add_edge(\"trend_agent\", \"check_logo\")\n",
    "\n",
    "# check_logo 이후의 분기는 edge_condition으로 구현할 수도 있고\n",
    "# node 내부에서 다음 노드를 결정하게 해도 됨.\n",
    "# 여기서는 간단히 \"무조건 select_logo로 가고, 거기서 has_logo False면 패스\"하는 패턴.\n",
    "builder.add_edge(\"check_logo\", \"select_logo\")\n",
    "builder.add_edge(\"select_logo\", \"build_prompt\")\n",
    "builder.add_edge(\"build_prompt\", \"generate_video\")\n",
    "builder.add_edge(\"generate_video\", \"confirm_save\")\n",
    "builder.add_edge(\"confirm_save\", \"save_to_db\")\n",
    "builder.add_edge(\"save_to_db\", END)\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "shortform_graph = builder.compile(checkpointer=checkpointer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca178e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ebdb69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb80f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a4615c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94afede7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e57465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ac552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2515270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (phase1_venv)",
   "language": "python",
   "name": "phase1_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
